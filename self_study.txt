##########  Documnet to reflect self studied materials along course ##########


1. PyTorch and modules:
    On the beggining of customn GPT2 implementation was unfamiliar with PyTorch documentation.
    Decided to step ahead and read documentation about:
    nn.Modules(https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) - Base class for all neural network module,
    nn.Embedding(https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding) - This module is often used to store word embeddings and retrieve them using indices,
    nn.LayerNorm(https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm) - Applies Layer Normalization over a mini-batch of inputs,
    nn.Linear(https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) - Applies an affine linear transformation to the incoming data
    nn.GELU(https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#torch.nn.GELU) - Activation function
    torch.arange(https://pytorch.org/docs/stable/generated/torch.arange.html#torch-arange) - Returns a 1-D tensor of size 

2. Papers:
Attention is all you need (https://arxiv.org/pdf/1706.03762) - Fundamental paper in transformers and attention
    

3. Problems:

# TODO Not working on MPS thing to research
# import torch._dynamo
# torch._dynamo.config.suppress_errors = True
# model = torch.compile(model)